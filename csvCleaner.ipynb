{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dfed59de",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import us\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b59f45f",
   "metadata": {},
   "source": [
    "Housing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ece80777",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_housing(csv):\n",
    "    # read in the csv file\n",
    "    df = pd.read_csv(csv)\n",
    "\n",
    "\n",
    "\n",
    "    # combine state and county fips and pad with zeros on left to ensure they are 5 digits long. Add a new column called 'FIPS' to the dataframe\n",
    "    df['FIPS'] = df['state'].astype(str).str.zfill(2) + df['county'].astype(str).str.zfill(3)\n",
    "    # drop STATE_FIPS and COUNTY_FIPS columns\n",
    "    new_df = df.drop(columns=['state', 'county'])\n",
    "\n",
    "\n",
    "\n",
    "    # find the column with the highest count for each row\n",
    "    age_columns = [col for col in new_df.columns if col.startswith('built_')]\n",
    "    new_df['MODAL_YEAR_BUILT_BIN'] = new_df[age_columns].idxmax(axis=1)\n",
    "    # drop all other house age columns\n",
    "    new_df = new_df[['FIPS', 'MODAL_YEAR_BUILT_BIN', 'MEDIAN_YEAR_BUILT']]\n",
    "    \n",
    "    return new_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4f4fac83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test = clean_housing(r'rawData\\2023\\county_house_age_2023.csv')\n",
    "# test.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0553817b",
   "metadata": {},
   "source": [
    "Storm Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ccf8fa7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_storm(csv):\n",
    "    # NOTE: adjust for inflation\n",
    "    df = pd.read_csv(csv)\n",
    "\n",
    "\n",
    "\n",
    "    # keep columns: first 6 columns, state, state_fips, month_name, event_type,cz_fips cz_name, damage_property, begin_lat, begin_lon, end_lat, end_lon. All names are in uppper case\n",
    "    new_df = df.iloc[:, :6]\n",
    "    join_df = df[['STATE', 'STATE_FIPS', 'MONTH_NAME', 'EVENT_TYPE', 'CZ_FIPS', 'CZ_NAME', 'DAMAGE_PROPERTY', 'BEGIN_LAT', 'BEGIN_LON', 'END_LAT', 'END_LON', 'BEGIN_DAY']]\n",
    "    new_df = pd.concat([new_df, join_df], axis=1)\n",
    "\n",
    "\n",
    "\n",
    "    # calculate the approximate area of the storm by using the haversine formula to calculate the distance between the begin and end coordinates. Add a new column called 'STORM_AREA' to the dataframe\n",
    "    # drop na values in cordinates columns\n",
    "    new_df = new_df.dropna(subset=['BEGIN_LAT', 'BEGIN_LON', 'END_LAT', 'END_LON'])\n",
    "    def rectagular_area(begin_lat, begin_lon, end_lat, end_lon):\n",
    "        # convert decimal degrees to radians\n",
    "        lat1, lon1, lat2, lon2 = map(np.radians, [begin_lat, begin_lon, end_lat, end_lon])\n",
    "\n",
    "        # use A = R² (sin lat1 − sin lat2) (lon1 − lon2).\n",
    "        # from https://www.johndcook.com/blog/2023/02/21/sphere-grid-area/#:~:text=Area%20of%20latitude/longitude%20grid&text=A%20=%20π%20R²%20(sin%20φ,1%20−%20θ2)/180.\n",
    "        r = 3956  # Radius of earth in miles\n",
    "        area = r**2 * (np.sin(lat1) - np.sin(lat2)) * (lon1 - lon2)\n",
    "        return abs(area)\n",
    "    new_df['STORM_AREA_SQMILES'] = new_df.apply(lambda row: rectagular_area(row['BEGIN_LAT'], row['BEGIN_LON'], row['END_LAT'], row['END_LON']), axis=1)\n",
    "\n",
    "    # drop the begin and end lat and lon columns\n",
    "    new_df = new_df.drop(columns=['BEGIN_LAT', 'BEGIN_LON', 'END_LAT', 'END_LON'])\n",
    "\n",
    "\n",
    "\n",
    "    # calculate the total damage by converting the damage property column to a numeric value. \n",
    "    # The damage property column is in the format of a string with a number followed by a letter (K, M, B) which represents the magnitude of the damage. \n",
    "    # keep missing values\n",
    "    def convert_damage(damage):\n",
    "        if pd.isna(damage):\n",
    "            return np.nan\n",
    "        elif damage.endswith('K'):\n",
    "            return float(damage[:-1]) * 1e3\n",
    "        elif damage.endswith('M'):\n",
    "            return float(damage[:-1]) * 1e6\n",
    "        elif damage.endswith('B'):\n",
    "            return float(damage[:-1]) * 1e9\n",
    "    new_df['DAMAGE_PROPERTY'] = new_df['DAMAGE_PROPERTY'].apply(convert_damage)\n",
    "\n",
    "\n",
    "    \n",
    "    # calculate the duration of the storm by using begin time and end time columns which are in military time (hhmm)\n",
    "    def calculate_duration(row):\n",
    "        begin_time = row['BEGIN_TIME']\n",
    "        end_time = row['END_TIME']\n",
    "\n",
    "        # pad the time strings with zeros if they are less than 4 characters long\n",
    "        begin_time = str(begin_time).zfill(4)\n",
    "        end_time = str(end_time).zfill(4)\n",
    "\n",
    "        begin_hours = int(begin_time[:2]) \n",
    "        end_hours = int(end_time[:2]) \n",
    "        begin_minutes = int(begin_time[2:])\n",
    "        end_minutes = int(end_time[2:])\n",
    "\n",
    "        duration = (end_hours * 60 + end_minutes) - (begin_hours * 60 + begin_minutes)\n",
    "        if duration < 0:\n",
    "            duration += 24 * 60  # Adjust for storms that last past midnight\n",
    "        return duration  # Return duration in minutes\n",
    "    new_df['DURATION_MINUTES'] = new_df.apply(calculate_duration, axis=1)\n",
    "\n",
    "\n",
    "\n",
    "    # keep event types: ones with flood in the name, Hail, heavy rain, high wind, lightning, strong wind, thunderstorm wind, and tornado\n",
    "    new_df = new_df[new_df['EVENT_TYPE'].str.contains('FLOOD|HAIL|HEAVY RAIN|HIGH WIND|LIGHTNING|STRONG WIND|THUNDERSTORM WIND|TORNADO', case=False, na=False)]\n",
    "    # drop marine event types\n",
    "    new_df = new_df[~new_df['EVENT_TYPE'].str.contains('Marine', case=False, na=False)]\n",
    "\n",
    "\n",
    "\n",
    "    # get month from yearmonth column and add it as a new column called 'MONTH'\n",
    "    new_df['MONTH'] = new_df['BEGIN_YEARMONTH'].astype(str).str[4:6].astype(int)\n",
    "    # drop first 6 columns\n",
    "    new_df = new_df.drop(columns=new_df.columns[:6])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # combine state cz fips and pad with zeros on left to ensure they are 5 digits long. Add a new column called 'FIPS' to the dataframe\n",
    "    new_df['FIPS'] = new_df['STATE_FIPS'].astype(str).str.zfill(2) + new_df['CZ_FIPS'].astype(str).str.zfill(3)\n",
    "    # drop STATE_FIPS and CZ_FIPS columns\n",
    "    new_df = new_df.drop(columns=['STATE_FIPS', 'CZ_FIPS'])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # keep only states in the continental US\n",
    "    non_continental_states = ['ALASKA', 'HAWAII', 'PUERTO RICO', 'GUAM', 'VIRGIN ISLANDS', 'AMERICAN SAMOA', 'NORTHERN MARIANA ISLANDS'] # keeping the district of columbia\n",
    "    new_df = new_df[~new_df['STATE'].isin(non_continental_states)]\n",
    "\n",
    "    # capitalize MONTH_NAME\n",
    "    new_df['MONTH_NAME'] = new_df['MONTH_NAME'].str.upper()\n",
    "\n",
    "    return new_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "16a1ffe4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test = clean_storm(r\"rawData\\2023\\StormData_2023.csv\")\n",
    "# test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80479fec",
   "metadata": {},
   "source": [
    "Population Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "924c4aa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_population(csv):\n",
    "    df = pd.read_csv(csv)\n",
    "    \n",
    "    # combine state and county fips and pad with zeros on left to ensure they are 5 digits long. Add a new column called 'FIPS' to the dataframe\n",
    "    df['FIPS'] = df['state'].astype(str).str.zfill(2) + df['county'].astype(str).str.zfill(3)\n",
    "    # drop STATE_FIPS and COUNTY_FIPS columns\n",
    "    new_df = df.drop(columns=['state', 'county'])\n",
    "\n",
    "    # drop all columns except FIPS and population\n",
    "    new_df = new_df[['FIPS', 'population']]\n",
    "\n",
    "    # capitalize the population column name\n",
    "    new_df = new_df.rename(columns={'population': 'POPULATION'})\n",
    "\n",
    "    return new_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "17713fb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_population = clean_population(r\"rawData\\2023\\county_population_2023.csv\")\n",
    "# test_population.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39e66711",
   "metadata": {},
   "source": [
    "Median Income"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fd102415",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_income(csv):\n",
    "    df = pd.read_csv(csv)\n",
    "\n",
    "    # combine state and county fips and pad with zeros on left to ensure they are 5 digits long. Add a new column called 'FIPS' to the dataframe\n",
    "    df['FIPS'] = df['state'].astype(str).str.zfill(2) + df['county'].astype(str).str.zfill(3)\n",
    "    # drop STATE_FIPS and COUNTY_FIPS columns\n",
    "    new_df = df.drop(columns=['state', 'county'])\n",
    "\n",
    "    # drop negative income values\n",
    "    new_df = new_df[new_df['MedianIncome'] >= 0]\n",
    "\n",
    "    # drop all columns except FIPS and median income\n",
    "    new_df = new_df[['FIPS', 'MedianIncome']]\n",
    "\n",
    "    # capitalize the median income column name\n",
    "    new_df = new_df.rename(columns={'MedianIncome': 'MEDIAN_INCOME'})\n",
    "    \n",
    "    return new_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "677cb8c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_income = clean_income(r\"rawData\\2023\\county_median_household_income_2023.csv\")\n",
    "# test_income.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eee7a05b",
   "metadata": {},
   "source": [
    "Relative Oceanic Nino Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2c04249b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_roni(csv):\n",
    "    # read in the csv file\n",
    "    df = pd.read_csv(csv)\n",
    "\n",
    "    # set the first column to Month\n",
    "    df = df.rename(columns={df.columns[0]: 'MONTH_NAME'})\n",
    "\n",
    "    df = df.rename(columns = {df.columns[1]: 'RONI_AVG'})\n",
    "\n",
    "    \n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "98c33396",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_roni = clean_roni(r\"rawData\\2023\\RONI_2023.csv\")\n",
    "# test_roni"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "479bc672",
   "metadata": {},
   "source": [
    "Temperature Anomoly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8942bf39",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_anomaly(csv):\n",
    "    df = pd.read_csv(csv)\n",
    "\n",
    "    # split partial fips data into two columns on \"-\"\n",
    "    df[['STATE_FIPS', 'COUNTY_FIPS']] = df[df.columns[0]].str.split('-', expand=True)\n",
    "\n",
    "    # drop partial fips column\n",
    "    df = df.drop(columns=df.columns[0])\n",
    "\n",
    "    # convert state abreviation to state number using a mapping dictionary\n",
    "    def abbreviation_to_fips(abbreviation):\n",
    "        \"\"\"Converts a state abbreviation to its FIPS number using the 'us' package.\"\"\"\n",
    "        state = us.states.lookup(abbreviation)\n",
    "        if state:\n",
    "            # state.fips returns the FIPS code as a string, convert to int if needed\n",
    "            return int(state.fips)\n",
    "        else:\n",
    "            return \"Invalid abbreviation\"\n",
    "    df['STATE_FIPS'] = df['STATE_FIPS'].apply(abbreviation_to_fips)\n",
    "\n",
    "    # combine state and county fips and pad with zeros on left to ensure they are 5 digits long. Add a new column called 'FIPS' to the dataframe\n",
    "    df['FIPS'] = df['STATE_FIPS'].astype(str).str.zfill(2) + df['COUNTY_FIPS'].astype(str).str.zfill(3)\n",
    "\n",
    "    # drop the first column and the state and county fips columns\n",
    "    df = df.drop(columns=['STATE_FIPS', 'COUNTY_FIPS'])\n",
    "\n",
    "    # change the column starting with Anomaly to just ANOMALY\n",
    "    df = df.rename(columns= {[col for col in df.columns if col.startswith('Anomaly')][0]: 'ANOMALY'})\n",
    "\n",
    "    # add farenheight to end of temperature column name\n",
    "    df = df.rename(columns={\"TEMPERATURE\": 'TEMPERATURE_F'})\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "af54919b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_anomaly = clean_anomaly(r\"rawData\\2023\\temperature_anomalies_2023.csv\")\n",
    "# test_anomaly.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "29693d9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_coastalTypes(csv):\n",
    "    df = pd.read_csv(csv)\n",
    "\n",
    "    # fill missing values in COASTAL_TYPE_SHORELINE with inland\n",
    "    df[\"COASTAL_TYPE_SHORELINE\"] = df[\"COASTAL_TYPE_SHORELINE\"].fillna(\"inland\")\n",
    "\n",
    "    # drop columns starting with NAME\n",
    "    df = df.drop(columns=[col for col in df.columns if col.startswith(\"NAME\")], errors='ignore')\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "155f4112",
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean the csv files and save them to a new folder called \"cleanedData\"\n",
    "year = 2023\n",
    "final_dir = f\"cleanedData/{year}\"\n",
    "if not os.path.exists(final_dir):\n",
    "    os.makedirs(final_dir)\n",
    "\n",
    "# clean and save the housing data\n",
    "cleaned_housing = clean_housing(f'rawData/{year}/county_house_age_{year}.csv')\n",
    "cleaned_housing.to_csv(f'{final_dir}/county_house_age.csv', index=False)\n",
    "\n",
    "# clean and save the storm data\n",
    "cleaned_storm = clean_storm(f'rawData/{year}/StormData_{year}.csv')\n",
    "cleaned_storm.to_csv(f'{final_dir}/StormData.csv', index=False)\n",
    "\n",
    "# clean and save the population data\n",
    "cleaned_population = clean_population(f'rawData/{year}/county_population_{year}.csv')\n",
    "cleaned_population.to_csv(f'{final_dir}/county_population.csv', index=False)\n",
    "\n",
    "# clean and save the income data\n",
    "cleaned_income = clean_income(f'rawData/{year}/county_median_household_income_{year}.csv')\n",
    "cleaned_income.to_csv(f'{final_dir}/county_median_household_income.csv', index=False)\n",
    "\n",
    "# clean and save the roni data\n",
    "cleaned_roni = clean_roni(f'rawData/{year}/RONI_{year}.csv')\n",
    "cleaned_roni.to_csv(f'{final_dir}/RONI.csv', index=False)\n",
    "\n",
    "# clean and save the anomaly data\n",
    "cleaned_anomaly = clean_anomaly(f'rawData/{year}/temperature_anomalies_{year}.csv')\n",
    "cleaned_anomaly.to_csv(f'{final_dir}/temperature_anomalies.csv', index=False)\n",
    "\n",
    "# clean and save the coastal types data\n",
    "cleaned_coastalTypes = clean_coastalTypes(f'rawData/{year}/coastal_counties_2010.csv')\n",
    "cleaned_coastalTypes.to_csv(f'{final_dir}/coastal_counties.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pyt313",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
