{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f48eed7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import requests\n",
    "import re\n",
    "import os\n",
    "from os import getenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "72239e55",
   "metadata": {},
   "outputs": [],
   "source": [
    "outDIR = r\"rawData\\2023\"\n",
    "if not os.path.exists(outDIR):\n",
    "    os.makedirs(outDIR)\n",
    "censusAPI = getenv(\"CENSUS_API_KEY\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84a9132e",
   "metadata": {},
   "source": [
    "[Census acs5 api attributes table for 2023](https://api.census.gov/data/2023/acs/acs5/variables.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9a3e63e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Population_csv(year, api_key, out_dir):\n",
    "        \"\"\"\n",
    "        Download ACS 5-year county-level population data for a given year\n",
    "        and save it as a CSV.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        year : int\n",
    "                ACS 5-year end year (e.g., 2019, 2020, 2021, 2022)\n",
    "        api_key : str\n",
    "                Census API key\n",
    "        out_dir : str\n",
    "                Output directory for CSV files\n",
    "        \"\"\"\n",
    "\n",
    "        url = f\"https://api.census.gov/data/{year}/acs/acs5\"\n",
    "        params = {\n",
    "                \"get\": \"NAME,B01003_001E\",\n",
    "                \"for\": \"county:*\",\n",
    "                \"key\": api_key\n",
    "        }\n",
    "        response = requests.get(url, params=params)\n",
    "        response.raise_for_status()\n",
    "\n",
    "        data = response.json()\n",
    "        df = pd.DataFrame(data[1:], columns=data[0])\n",
    "\n",
    "        df.rename(columns={\n",
    "                \"B01003_001E\": \"population\"\n",
    "        }, inplace=True)\n",
    "\n",
    "        output_file = f\"{out_dir}/county_population_{year}.csv\"\n",
    "        df.to_csv(output_file, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "313dd190",
   "metadata": {},
   "outputs": [],
   "source": [
    "Population_csv(2023, censusAPI, outDIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ada6c04a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def MedianIncome_csv(year, api_key, out_dir):\n",
    "    \"\"\"\n",
    "    Download ACS 5-year county-level median household income for a given year\n",
    "    and save it as a CSV. The median household income is inflation-adjusted to the given year.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    year : int\n",
    "        ACS 5-year estimate year (must be between 2009 and 2023)\n",
    "    api_key : str\n",
    "        Census API key\n",
    "    out_dir : str\n",
    "        Directory where the CSV will be written\n",
    "\n",
    "    Output\n",
    "    ------\n",
    "    Writes a CSV named:\n",
    "    county_median_household_income_<year>.csv\n",
    "    \"\"\"\n",
    "\n",
    "    url = f\"https://api.census.gov/data/{year}/acs/acs5\"\n",
    "    params = {\n",
    "        \"get\": \"NAME,B19013_001E\",\n",
    "        \"for\": \"county:*\",\n",
    "        \"in\": \"state:*\",\n",
    "        \"key\": api_key\n",
    "    }\n",
    "\n",
    "    response = requests.get(url, params=params)\n",
    "    response.raise_for_status()\n",
    "\n",
    "    data = response.json()\n",
    "    df = pd.DataFrame(data[1:], columns=data[0])\n",
    "\n",
    "    df.rename(columns={\"B19013_001E\": \"MedianIncome\"}, inplace=True)\n",
    "\n",
    "    output_file = f\"{out_dir}/county_median_household_income_{year}.csv\"\n",
    "    df.to_csv(output_file, index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d13dbe0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "MedianIncome_csv(2023, censusAPI, outDIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bb87e766",
   "metadata": {},
   "outputs": [],
   "source": [
    "def HouseAge_csv(year, api_key, out_dir):\n",
    "    \"\"\"\n",
    "    Downloads ACS 5-year county-level home age data (Table B25034)\n",
    "    for a given year and saves it as a CSV.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    year : int\n",
    "        ACS 5-year end year (e.g., 2019, 2020, 2021, 2022)\n",
    "    api_key : str\n",
    "        Census API key\n",
    "    out_dir : str\n",
    "        Output directory for CSV files\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "    # ACS 5-year endpoint\n",
    "    base_url = f\"https://api.census.gov/data/{year}/acs/acs5\"\n",
    "\n",
    "    # B25034: Year structure built\n",
    "    variables = [\n",
    "        \"NAME\",\n",
    "        \"B25034_001E\",  # Total housing units\n",
    "        \"B25034_002E\",  # Built 2020 or later (recent years vary by ACS year)\n",
    "        \"B25034_003E\",  # Built 2010 to 2019\n",
    "        \"B25034_004E\",  # Built 2000 to 2009\n",
    "        \"B25034_005E\",  # Built 1990 to 1999\n",
    "        \"B25034_006E\",  # Built 1980 to 1989\n",
    "        \"B25034_007E\",  # Built 1970 to 1979\n",
    "        \"B25034_008E\",  # Built 1960 to 1969\n",
    "        \"B25034_009E\",  # Built 1950 to 1959\n",
    "        \"B25034_010E\",  # Built 1940 to 1949\n",
    "        \"B25035_001E\"   # Median age of housing units\n",
    "    ]\n",
    "\n",
    "    params = {\n",
    "        \"get\": \",\".join(variables),\n",
    "        \"for\": \"county:*\",\n",
    "        \"in\": \"state:*\",\n",
    "        \"key\": api_key\n",
    "    }\n",
    "\n",
    "    response = requests.get(base_url, params=params)\n",
    "    response.raise_for_status()\n",
    "\n",
    "    data = response.json()\n",
    "\n",
    "    df = pd.DataFrame(data[1:], columns=data[0])\n",
    "\n",
    "    # Convert numeric columns\n",
    "    for col in variables[1:12]:\n",
    "        df[col] = pd.to_numeric(df[col], errors=\"coerce\")\n",
    "\n",
    "    # rename columns for clarity\n",
    "    df.rename(columns={\n",
    "        \"B25034_001E\": \"total_housing_units\",\n",
    "        \"B25034_002E\": \"built_2020_or_later\",\n",
    "        \"B25034_003E\": \"built_2010_to_2019\",\n",
    "        \"B25034_004E\": \"built_2000_to_2009\",\n",
    "        \"B25034_005E\": \"built_1990_to_1999\",\n",
    "        \"B25034_006E\": \"built_1980_to_1989\",\n",
    "        \"B25034_007E\": \"built_1970_to_1979\",\n",
    "        \"B25034_008E\": \"built_1960_to_1969\",\n",
    "        \"B25034_009E\": \"built_1950_to_1959\",\n",
    "        \"B25034_010E\": \"built_1940_to_1949\",\n",
    "        \"B25035_001E\": \"MEDIAN_YEAR_BUILT\"\n",
    "    }, inplace=True)\n",
    "\n",
    "    output_file = f\"{out_dir}/county_house_age_{year}.csv\"\n",
    "    df.to_csv(output_file, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9d3dbb8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "HouseAge_csv(2023, censusAPI, outDIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2bcb98a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def RONI_csv(year,out_dir,rawData):\n",
    "\n",
    "    # read in the csv file\n",
    "    df = pd.read_csv(rawData)\n",
    "\n",
    "    # set the year as the index\n",
    "    df = df.set_index('Year')\n",
    "\n",
    "    # rename the columns DJF becomes december, january, and february\n",
    "    df = df.rename(columns={'DJF': 'DECEMBER_JANUARY_FEBRUARY'})\n",
    "    df = df.rename(columns={'JFM': 'JANUARY_FEBRUARY_MARCH'})\n",
    "    df = df.rename(columns={'FMA': 'FEBRUARY_MARCH_APRIL'})\n",
    "    df = df.rename(columns={'MAM': 'MARCH_APRIL_MAY'})\n",
    "    df = df.rename(columns={'AMJ': 'APRIL_MAY_JUNE'})\n",
    "    df = df.rename(columns={'MJJ': 'MAY_JUNE_JULY'})\n",
    "    df = df.rename(columns={'JJA': 'JUNE_JULY_AUGUST'})\n",
    "    df = df.rename(columns={'JAS': 'JULY_AUGUST_SEPTEMBER'})\n",
    "    df = df.rename(columns={'ASO': 'AUGUST_SEPTEMBER_OCTOBER'})\n",
    "    df = df.rename(columns={'SON': 'SEPTEMBER_OCTOBER_NOVEMBER'})\n",
    "    df = df.rename(columns={'OND': 'OCTOBER_NOVEMBER_DECEMBER'})\n",
    "    df = df.rename(columns={'NDJ': 'NOVEMBER_DECEMBER_JANUARY'})\n",
    "\n",
    "    # split each column with the month names into three separate columns with the month name as the column name and the value as the value.\n",
    "    # if month exists already add the value to the existing column.\n",
    "    month_columns = ['DECEMBER_JANUARY_FEBRUARY', 'JANUARY_FEBRUARY_MARCH', 'FEBRUARY_MARCH_APRIL', 'MARCH_APRIL_MAY', 'APRIL_MAY_JUNE', 'MAY_JUNE_JULY', 'JUNE_JULY_AUGUST', 'JULY_AUGUST_SEPTEMBER', 'AUGUST_SEPTEMBER_OCTOBER', 'SEPTEMBER_OCTOBER_NOVEMBER', 'OCTOBER_NOVEMBER_DECEMBER', 'NOVEMBER_DECEMBER_JANUARY']\n",
    "    for month_column in month_columns:\n",
    "        months = month_column.split('_')\n",
    "        for month in months:\n",
    "            col_name = month\n",
    "            if col_name in df.columns:\n",
    "                df[col_name] += df[month_column]\n",
    "            else:\n",
    "                df[col_name] = df[month_column]\n",
    "    \n",
    "        # drop the original column\n",
    "        df = df.drop(columns=[month_column])\n",
    "\n",
    "    # divide all month columns by 3 to get the average value for each month\n",
    "    df = df.div(3)\n",
    "\n",
    "    # get the row corresponding to the given year\n",
    "    df = df.loc[year]\n",
    "\n",
    "    # write the dataframe to a csv file\n",
    "    output_file = f\"{out_dir}/RONI_{year}.csv\"\n",
    "    df.to_csv(output_file)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "462492ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "RONI_csv(2023, outDIR, r\"RONI_data\\rawData.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "74e2a2ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stormDamage_csv(year, out_dir):\n",
    "        \"\"\"\n",
    "        Download NOAA Storm Events Database damage data for a given year\n",
    "        and save it as a CSV.\n",
    "        \"\"\"\n",
    "\n",
    "        # 1. Get the URL for the StormEvents details file for the given year\n",
    "        base_url = \"https://www.ncei.noaa.gov/pub/data/swdi/stormevents/csvfiles/\"\n",
    "        r = requests.get(base_url)\n",
    "        r.raise_for_status()\n",
    "        \n",
    "        pattern = re.compile(\n",
    "            fr'StormEvents_details-ftp_v1\\.0_d{year}_c\\d+\\.csv\\.gz'\n",
    "        )\n",
    "        match = pattern.search(r.text)\n",
    "        if match:\n",
    "            url = str(base_url + match.group(0))\n",
    "        else:\n",
    "            raise ValueError(f\"No StormEvents details file found for year {year}\")\n",
    "        \n",
    "        # 2. Download and read the CSV file into a DataFrame\n",
    "        df = pd.read_csv(url, compression='gzip', low_memory=False)\n",
    "\n",
    "\n",
    "        # 3. save the DataFrame as a CSV file in the specified output directory\n",
    "        output_file = f\"{out_dir}/StormData_{year}.csv\"\n",
    "        df.to_csv(output_file, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d381af81",
   "metadata": {},
   "outputs": [],
   "source": [
    "stormDamage_csv(2023, outDIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "39e01d6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tempAnomaly(year, out_dir):\n",
    "    \"\"\"\n",
    "    Downloads monthly county temperature CSVs for a full year from NOAA \n",
    "    and merges them into a single master file.\n",
    "    \"\"\"\n",
    "    base_url = \"https://www.ncei.noaa.gov/access/monitoring/climate-at-a-glance/county/mapping/110/tavg/\"\n",
    "    all_monthly_data = []\n",
    "\n",
    "    # make temporary directory for raw monthly files in output directory\n",
    "    temp_dir = f\"{out_dir}/temporary_monthly\"\n",
    "    os.makedirs(temp_dir, exist_ok=True)\n",
    "\n",
    "    for month in range(1, 13):\n",
    "        # Format month to YYYYMM (e.g., 202301)\n",
    "        date_code = f\"{year}{str(month).zfill(2)}\"\n",
    "        url = f\"{base_url}{date_code}.csv\"\n",
    "\n",
    "        response = requests.get(url)\n",
    "\n",
    "        if response.status_code == 200:\n",
    "            # Save raw file\n",
    "            file_path = f\"{temp_dir}/temperature_anomalies_{date_code}.csv\"\n",
    "            with open(file_path, \"wb\") as f:\n",
    "                f.write(response.content)\n",
    "\n",
    "            # Read the CSV into a DataFrame and add a 'month' column\n",
    "            monthly_df = pd.read_csv(file_path, skiprows=3)  # Skip metadata rows\n",
    "            monthly_df['MONTH'] = month\n",
    "\n",
    "            # change column that starts with location id to fips code\n",
    "            monthly_df['PartialFIPS'] = monthly_df['ID']\n",
    "\n",
    "            # change value column to temperature\n",
    "            monthly_df.rename(columns={'Value': 'TEMPERATURE'}, inplace=True)\n",
    "\n",
    "            all_monthly_data.append(monthly_df[['PartialFIPS','TEMPERATURE', 'Anomaly (1901-2000 base period)', 'MONTH']])\n",
    "    \n",
    "    # Merge all monthly DataFrames into a single master DataFrame\n",
    "    master_df = pd.concat(all_monthly_data, ignore_index=True)\n",
    "    master_df.to_csv(f\"{out_dir}/temperature_anomalies_{year}.csv\", index=False)\n",
    "\n",
    "    # delete temporary directory and raw monthly files\n",
    "    for month in range(1, 13):\n",
    "        date_code = f\"{year}{str(month).zfill(2)}\"\n",
    "        file_path = f\"{temp_dir}/temperature_anomalies_{date_code}.csv\"\n",
    "        os.remove(file_path)\n",
    "    os.rmdir(temp_dir)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "930e1e27",
   "metadata": {},
   "outputs": [],
   "source": [
    "tempAnomaly(2023, outDIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8b7be190",
   "metadata": {},
   "outputs": [],
   "source": [
    "def coastalType(out_dir):\n",
    "    \"\"\"\n",
    "    2010 shoreline and watershed county data from NOAA ArcGIS service. This is a static dataset that doesn't change by year, but we include the year parameter for consistency with other functions and to allow for future updates if needed.\n",
    "    \"\"\"\n",
    "\n",
    "    def arcgis_query_all(layer_query_url: str, out_fields: str):\n",
    "        \"\"\"\n",
    "        Helper for shoreline and watershed counties: Query an ArcGIS layer and return ALL records, handling pagination.\n",
    "        \"\"\"\n",
    "        rows = []\n",
    "        offset = 0\n",
    "        page_size = 2000  # service MaxRecordCount is 2000\n",
    "\n",
    "        while True:\n",
    "            params = {\n",
    "                \"where\": \"1=1\",\n",
    "                \"outFields\": out_fields,\n",
    "                \"returnGeometry\": \"false\",\n",
    "                \"f\": \"json\",\n",
    "                \"resultOffset\": offset,\n",
    "                \"resultRecordCount\": page_size,\n",
    "            }\n",
    "            r = requests.get(layer_query_url, params=params, timeout=120)\n",
    "            r.raise_for_status()\n",
    "            data = r.json()\n",
    "\n",
    "            # If ArcGIS returns an error, it will look like {\"error\": {...}}\n",
    "            if \"error\" in data:\n",
    "                raise RuntimeError(f\"ArcGIS error from {layer_query_url}: {data['error']}\")\n",
    "\n",
    "            feats = data.get(\"features\", [])\n",
    "            if not feats:\n",
    "                break\n",
    "\n",
    "            for f in feats:\n",
    "                rows.append(f.get(\"attributes\", {}))\n",
    "\n",
    "            offset += len(feats)\n",
    "\n",
    "            # Some services also include exceededTransferLimit; weâ€™re paginating anyway.\n",
    "            if len(feats) < page_size:\n",
    "                break\n",
    "\n",
    "        return pd.DataFrame(rows)\n",
    "\n",
    "    def ShorelineCounties_csv():\n",
    "        \"\"\"\n",
    "        Downloads shoreline counties from NOAA ArcGIS service\n",
    "        and saves it as a CSV.\n",
    "        \"\"\"\n",
    "\n",
    "        \n",
    "        SHORELINE_LAYER = \"https://maps1.coast.noaa.gov/arcgis/rest/services/Landcover/Coastal_County_Update_Review/MapServer/9/query\"\n",
    "\n",
    "        df = arcgis_query_all(\n",
    "            SHORELINE_LAYER,\n",
    "            out_fields=\"fips,cntyname,st_name\"\n",
    "        )\n",
    "\n",
    "        df = df.rename(columns={\n",
    "            \"fips\": \"FIPS\",\n",
    "            \"cntyname\": \"COUNTY\",\n",
    "            \"st_name\": \"STATE_NAME\",\n",
    "        })\n",
    "\n",
    "        df[\"FIPS\"] = df[\"FIPS\"].astype(str).str.zfill(5)\n",
    "        df[\"COASTAL_TYPE_SHORELINE\"] = \"shoreline\"\n",
    "\n",
    "        # combine names county and state into one column with format \"County, State\"\n",
    "        df[\"NAME\"] = df[\"COUNTY\"] + \", \" + df[\"STATE_NAME\"]\n",
    "\n",
    "        # only relevant columns\n",
    "        return df[[\"FIPS\", \"COASTAL_TYPE_SHORELINE\", \"NAME\"]]\n",
    "\n",
    "    def WatershedCounties_csv():\n",
    "        \"\"\"\n",
    "        Downloads watershed counties from NOAA ArcGIS service\n",
    "        and saves it as a CSV.\n",
    "        \"\"\"\n",
    "        \n",
    "        WATERSHED_LAYER = \"https://maps1.coast.noaa.gov/arcgis/rest/services/Landcover/Coastal_County_Update_Review/MapServer/33/query\"\n",
    "\n",
    "        df = arcgis_query_all(\n",
    "            WATERSHED_LAYER,\n",
    "            out_fields=\"fips,cntyname,st_name\"\n",
    "        )\n",
    "        df = df.rename(columns={\n",
    "            \"fips\": \"FIPS\",\n",
    "            \"cntyname\": \"COUNTY\",\n",
    "            \"st_name\": \"STATE_NAME\",\n",
    "        })\n",
    "\n",
    "        df[\"FIPS\"] = df[\"FIPS\"].astype(str).str.zfill(5)\n",
    "        df[\"COASTAL_TYPE_WATERSHED\"] = \"watershed\"\n",
    "\n",
    "        # combine names county and state into one column with format \"County, State\"\n",
    "        df[\"NAME\"] = df[\"COUNTY\"] + \", \" + df[\"STATE_NAME\"]\n",
    "\n",
    "        return df[[\"FIPS\", \"COASTAL_TYPE_WATERSHED\", \"NAME\"]]\n",
    "\n",
    "    shoreline_df = ShorelineCounties_csv()\n",
    "    watershed_df = WatershedCounties_csv()\n",
    "\n",
    "    # merge shoreline and watershed dataframes on FIPS code, keeping all rows (outer join)\n",
    "    df = pd.merge(shoreline_df, watershed_df, on=\"FIPS\", how=\"outer\", suffixes=(\"_SHORELINE\", \"_WATERSHED\"))\n",
    "    df.to_csv(f\"{out_dir}/coastal_counties_2010.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "dc01c92f",
   "metadata": {},
   "outputs": [],
   "source": [
    "coastalType(outDIR)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pyt313",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
